import os
import joblib
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.io as pio
import glob
from datetime import timedelta
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

import yfinance as yf



#LSTM

def train_lstm_model_from_csv(file_path, window_size=60, epochs=20, batch_size=32):
    PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
    selected_symbol = os.path.splitext(os.path.basename(file_path))[0]
    model_dir = os.path.join(PROJECT_ROOT, "models")
    os.makedirs(model_dir, exist_ok=True)

    model_save_path = os.path.join(
        model_dir, f"{selected_symbol}_lstm_ws{window_size}_ep{epochs}.h5"
    )
    scaler_save_path = os.path.join(
        model_dir, f"{selected_symbol}_lstm_scaler_ws{window_size}_ep{epochs}.pkl"
    )

    # Load data
    df = pd.read_csv(file_path)
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values('Date')
    data = df['Close'].values.reshape(-1, 1)

    # Scale
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data)

    # üëá Split BEFORE creating windows
    split_index = int(len(scaled_data) * 0.8)
    train_data = scaled_data[:split_index]
    test_data = scaled_data[split_index - window_size:]  # üëà c·∫ßn gi·ªØ l·∫°i `window_size` tr∆∞·ªõc test ƒë·ªÉ t·∫°o chu·ªói ƒë·∫ßy ƒë·ªß

    def create_dataset(series, window):
        X, y = [], []
        for i in range(len(series) - window):
            X.append(series[i:i + window])
            y.append(series[i + window])
        return np.array(X), np.array(y)

    X_train, y_train = create_dataset(train_data, window_size)
    X_test, y_test = create_dataset(test_data, window_size)

    X_train = X_train.reshape(-1, window_size, 1)
    X_test = X_test.reshape(-1, window_size, 1)

    # Build model
    model = Sequential([
        Bidirectional(LSTM(128, return_sequences=True), input_shape=(window_size, 1)),
        Dropout(0.2),
        Bidirectional(LSTM(64)),
        Dropout(0.2),
        Dense(1)
    ])

    model.compile(optimizer='adam', loss='mean_squared_error')
    early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)

    print(f"üîß ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Bidirectional LSTM cho {selected_symbol}...")
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stop])

    model.save(model_save_path)
    joblib.dump(scaler, scaler_save_path)
    model_type = "lstm"

    # D·ª± b√°o v√† t√≠nh l·ªói
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))
    y_train_pred_inv = scaler.inverse_transform(y_train_pred)
    y_test_pred_inv = scaler.inverse_transform(y_test_pred)

    mae = mean_absolute_error(y_test_inv, y_test_pred_inv)
    rmse = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))
    mape = np.mean(np.abs((y_test_inv - y_test_pred_inv) / y_test_inv)) * 100

    error_dir = os.path.join(PROJECT_ROOT, "errors")
    os.makedirs(error_dir, exist_ok=True)
    error_path = os.path.join(
        error_dir, f"{selected_symbol}_{model_type}_ws{window_size}_ep{epochs}_bs{batch_size}.csv"
    )
    pd.DataFrame([{
        "Symbol": selected_symbol,
        "Model": "LSTM",
        "Window Size": window_size,
        "Epochs": epochs,
        "Batch Size": batch_size,
        "MAE": mae,
        "RMSE": rmse,
        "MAPE": mape
    }]).to_csv(error_path, index=False)

    print(f"üìÅ ƒê√£ l∆∞u l·ªói d·ª± b√°o t·∫°i: {error_path}")
    print(f"‚úÖ ƒê√£ l∆∞u model t·∫°i: {model_save_path}")
    print(f"‚úÖ ƒê√£ l∆∞u scaler t·∫°i: {scaler_save_path}")




#GRU

def train_gru_model_from_csv(file_path, window_size=60, epochs=20, batch_size=32):
    PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
    selected_symbol = os.path.splitext(os.path.basename(file_path))[0]
    model_dir = os.path.join(PROJECT_ROOT, "models")
    os.makedirs(model_dir, exist_ok=True)

    model_save_path = os.path.join(
        model_dir, f"{selected_symbol}_gru_ws{window_size}_ep{epochs}.h5"
    )
    scaler_save_path = os.path.join(
        model_dir, f"{selected_symbol}_gru_scaler_ws{window_size}_ep{epochs}.pkl"
    )

    # Load data
    df = pd.read_csv(file_path)
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values('Date')
    data = df['Close'].values.reshape(-1, 1)

    # Scale
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data)

    def create_dataset(series, window_size):
        X, y = [], []
        for i in range(len(series) - window_size):
            X.append(series[i:i + window_size])
            y.append(series[i + window_size])
        return np.array(X), np.array(y)

    X, y = create_dataset(scaled_data, window_size)
    X = X.reshape(-1, window_size, 1)

    # Build model
    model = Sequential()
    model.add(GRU(64, input_shape=(window_size, 1)))
    model.add(Dropout(0.2))
    model.add(Dense(1))

    model.compile(optimizer='adam', loss='mean_squared_error')
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    print(f"üîß ƒêang hu·∫•n luy·ªán m√¥ h√¨nh GRU cho {selected_symbol}...")
    model.fit(X, y, validation_split=0.1,
              epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stop])

    model.save(model_save_path)
    joblib.dump(scaler, scaler_save_path)
    model_type = "gru"  # ho·∫∑c "lstm" t√πy v√†o h√†m
    split_index = int(len(X) * 0.8)
    X_train, y_train = X[:split_index], y[:split_index]
    X_test, y_test = X[split_index:], y[split_index:]

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))
    y_train_pred_inv = scaler.inverse_transform(y_train_pred)
    y_test_pred_inv = scaler.inverse_transform(y_test_pred)

    mae = mean_absolute_error(y_test_inv, y_test_pred_inv)
    rmse = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))
    mape = np.mean(np.abs((y_test_inv - y_test_pred_inv) / y_test_inv)) * 100

    # ‚úÖ T·∫°o th∆∞ m·ª•c errors n·∫øu ch∆∞a c√≥
    error_dir = os.path.join(PROJECT_ROOT, "errors")
    os.makedirs(error_dir, exist_ok=True)

    # ‚úÖ T√™n file ƒë·∫ßy ƒë·ªß theo format chu·∫©n
    error_path = os.path.join(
        error_dir, f"{selected_symbol}_{model_type}_ws{window_size}_ep{epochs}_bs{batch_size}.csv"
    )

    # ‚úÖ L∆∞u file l·ªói
    pd.DataFrame([{
        "Symbol": selected_symbol,
        "Model": model_type.upper(),  # ‚¨Ö d√πng bi·∫øn
        "Window Size": window_size,
        "Epochs": epochs,
        "Batch Size": batch_size,
        "MAE": mae,
        "RMSE": rmse,
        "MAPE": mape
    }]).to_csv(error_path, index=False)

    print(f"üìÅ ƒê√£ l∆∞u l·ªói d·ª± b√°o t·∫°i: {error_path}")
    print(f"üìÅ ƒê√£ l∆∞u l·ªói d·ª± b√°o t·∫°i: {error_path}")
    print(f"‚úÖ ƒê√£ l∆∞u GRU model t·∫°i: {model_save_path}")
    print(f"‚úÖ ƒê√£ l∆∞u GRU scaler t·∫°i: {scaler_save_path}")





#Bi·ªÉu ƒë·ªì v√† ch·ªâ s·ªë l·ªói
def analyze_trained_model(symbol, model_type='lstm', window_size=60, epochs=20):
    PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
    model_type = model_type.lower()
    symbol = symbol.upper()

    model_file = f"{symbol}_{model_type}_ws{window_size}_ep{epochs}.h5"
    scaler_file = f"{symbol}_{model_type}_scaler_ws{window_size}_ep{epochs}.pkl"

    model_path = os.path.join(PROJECT_ROOT, 'models', model_file)
    scaler_path = os.path.join(PROJECT_ROOT, 'models', scaler_file)
    csv_path = os.path.join(PROJECT_ROOT, 'dataset', f"{symbol}.csv")

    if not (os.path.exists(model_path) and os.path.exists(scaler_path) and os.path.exists(csv_path)):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y model, scaler ho·∫∑c dataset.\n{model_path}\n{scaler_path}\n{csv_path}")
    # Load d·ªØ li·ªáu
    df = pd.read_csv(csv_path)
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values('Date')
    data = df['Close'].values.reshape(-1, 1)

    # Load model + scaler
    model = load_model(model_path)
    scaler = joblib.load(scaler_path)

    # Scale l·∫°i
    scaled_data = scaler.transform(data)

    # T·∫°o t·∫≠p d·ªØ li·ªáu
    def create_dataset(series, window):
        X, y = [], []
        for i in range(len(series) - window):
            X.append(series[i:i + window])
            y.append(series[i + window])
        return np.array(X), np.array(y)

    X, y = create_dataset(scaled_data, window_size)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    X_train = X_train.reshape(-1, window_size, 1)
    X_test = X_test.reshape(-1, window_size, 1)

    # D·ª± ƒëo√°n
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))
    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))
    y_train_pred_inv = scaler.inverse_transform(y_train_pred)
    y_test_pred_inv = scaler.inverse_transform(y_test_pred)

    # C√°c m·ªëc th·ªùi gian
    train_dates = df['Date'].iloc[window_size:split_idx + window_size].dropna()
    test_dates = df['Date'].iloc[split_idx + window_size:].dropna()

    # T·∫°o bi·ªÉu ƒë·ªì Plotly
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=train_dates, y=y_train_inv.flatten(), mode='lines', name='Train Real', line=dict(color='#38bdf8')))
    fig.add_trace(go.Scatter(x=train_dates, y=y_train_pred_inv.flatten(), mode='lines', name='Train Predict', line=dict(color='#facc15')))
    fig.add_trace(go.Scatter(x=test_dates, y=y_test_inv.flatten(), mode='lines', name='Test Real', line=dict(color='#22c55e')))
    fig.add_trace(go.Scatter(x=test_dates, y=y_test_pred_inv.flatten(), mode='lines', name='Test Predict', line=dict(color='#ef4444')))
    fig.update_layout(
        title=f"{model_type.upper()} Model Analysis - {symbol}",
        xaxis_title="Date",
        yaxis_title="Price",
        template="plotly_dark",
        height=480,
        margin=dict(l=20, r=20, t=40, b=20)
    )
    chart_html = pio.to_html(fig, full_html=False, include_plotlyjs='cdn')

    # Ch·ªâ s·ªë l·ªói TRAIN & TEST
    metrics_train = {
        "MAE": float(mean_absolute_error(y_train_inv, y_train_pred_inv)),
        "RMSE": float(np.sqrt(mean_squared_error(y_train_inv, y_train_pred_inv))),
        "MAPE": float(np.mean(np.abs((y_train_inv - y_train_pred_inv) / y_train_inv)) * 100)
    }
    metrics_test = {
        "MAE": float(mean_absolute_error(y_test_inv, y_test_pred_inv)),
        "RMSE": float(np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))),
        "MAPE": float(np.mean(np.abs((y_test_inv - y_test_pred_inv) / y_test_inv)) * 100)
    }

    # Tr·∫£ v·ªÅ dict ƒë√∫ng chu·∫©n Jinja2 template
    return {
        'model_type': model_type.upper(),
        'symbol': symbol,
        'metrics_train': metrics_train,
        'metrics_test': metrics_test,
        'chart_html': chart_html,
        'summary': "ƒê√°nh gi√° m√¥ h√¨nh d·ª±a tr√™n t·∫≠p Train v√† Test."
    }




def predict_future(df, scaler, window_size=60, steps=10, symbol="AAPL", model_type="lstm", model_path=None):
    print("DEBUG: df.shape =", df.shape)
    print("DEBUG: df.columns =", df.columns)
    print("DEBUG: df.head() =", df.head())
    model_type = model_type.lower()
    if model_type not in ["lstm", "gru"]:
        raise ValueError("‚ùå model_type ph·∫£i l√† 'lstm' ho·∫∑c 'gru'")

    # ‚úÖ T√¨m model m·ªõi nh·∫•t n·∫øu kh√¥ng truy·ªÅn v√†o
    if model_path is None:
        model_files = glob.glob(f"models/{symbol}_{model_type}_ws*.h5")
        if not model_files:
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh: models/{symbol}_{model_type}_ws*.h5")
        model_path = max(model_files, key=os.path.getmtime)

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh t·∫°i {model_path}")
    model = load_model(model_path)

    if 'Close' not in df.columns or len(df) < window_size:
        print(f"‚ùå D·ªØ li·ªáu kh√¥ng ƒë·ªß. df c√≥ {len(df)} d√≤ng, c·∫ßn √≠t nh·∫•t {window_size} v·ªõi c·ªôt 'Close'.")
        return pd.DataFrame([])

    last_sequence = df['Close'].values[-window_size:]
    last_scaled = scaler.transform(last_sequence.reshape(-1, 1))
    input_seq = last_scaled.reshape(1, window_size, 1)
    last_date = pd.to_datetime(df['Date']).max()
    forecast = []

    for _ in range(steps):
        pred_scaled = model.predict(input_seq, verbose=0)
        pred_price = scaler.inverse_transform(pred_scaled)[0][0]

        next_date = last_date + timedelta(days=1)
        while next_date.weekday() >= 5:
            next_date += timedelta(days=1)

        forecast.append({"Date": next_date, "Close": round(pred_price, 2)})
        last_date = next_date
        input_seq = np.append(input_seq[:, 1:, :], pred_scaled.reshape(1, 1, 1), axis=1)

    forecast_df = pd.DataFrame(forecast)[["Date", "Close"]]
    print("DEBUG: forecast_df =", forecast_df)
    return forecast_df



def plot_forecast_chart(df, forecast_df):
    # df: DataFrame g·ªëc (c√≥ Date, Close)
    # forecast_df: DataFrame d·ª± b√°o (c√≥ Date, Close)

    df = df.copy()
    df['Date'] = pd.to_datetime(df['Date'])

    # D·ªØ li·ªáu th·ª±c t·∫ø
    trace_actual = go.Scatter(
        x=df['Date'], y=df['Close'],
        mode='lines', name='Actual Price', line=dict(color='royalblue')
    )
    # D·ª± b√°o
    forecast_dates = forecast_df['Date']
    forecast_prices = forecast_df['Close']
    trace_pred = go.Scatter(
        x=forecast_dates, y=forecast_prices,
        mode='lines+markers', name='Forecast', line=dict(color='orangered', dash='dash')
    )

    layout = go.Layout(
        title='Price Forecast',
        xaxis_title='Date',
        yaxis_title='Price',
        template='plotly_dark'
    )
    fig = go.Figure([trace_actual, trace_pred], layout)
    chart_html = fig.to_html(full_html=False, include_plotlyjs='cdn')
    return chart_html


def log(message):
    print(f"[LOG] {message}")

def evaluate_with_yahoo(symbol, model_type='lstm', window_size=60, steps=30):
    try:
        log(f"üöÄ ƒêang ƒë√°nh gi√° m√¥ h√¨nh {model_type.upper()} cho {symbol}...")

        model_path = os.path.join("models", f"{symbol}_{model_type}_model.h5")
        scaler_path = os.path.join("models", f"{symbol}_{model_type}_scaler.pkl")
        if not os.path.exists(model_path) or not os.path.exists(scaler_path):
            raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y model ho·∫∑c scaler ƒë√£ hu·∫•n luy·ªán.")

        log("‚úÖ T·∫£i model v√† scaler...")
        model = load_model(model_path)
        scaler = joblib.load(scaler_path)

        log("üì• ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Yahoo Finance...")
        df_yahoo = yf.download(symbol, start="2022-07-12", end="2022-09-30", auto_adjust=True)
        df_yahoo.reset_index(inplace=True)
        df_yahoo = df_yahoo[['Date', 'Close']].dropna()
        df_yahoo['Date'] = pd.to_datetime(df_yahoo['Date'])

        log("üìä T·∫°o d·ªØ li·ªáu d·ª± b√°o t∆∞∆°ng lai...")
        last_sequence = df_yahoo['Close'].values[-window_size:]
        last_scaled = scaler.transform(last_sequence.reshape(-1, 1))
        input_seq = last_scaled.reshape(1, window_size, 1)
        last_date = df_yahoo['Date'].max()

        forecast = []
        for i in range(steps):
            pred = model.predict(input_seq, verbose=0)
            pred_price = scaler.inverse_transform(pred)[0][0]
            next_date = last_date + timedelta(days=1)
            while next_date.weekday() >= 5:
                next_date += timedelta(days=1)
            forecast.append((next_date, pred_price))
            last_date = next_date
            input_seq = np.append(input_seq[:, 1:, :], pred.reshape(1, 1, 1), axis=1)

        log(f"üìÖ ƒê√£ t·∫°o {len(forecast)} ng√†y d·ª± b√°o.")

        # Gh√©p tay v·ªõi th·ª±c t·∫ø
        actual_dict = df_yahoo.set_index('Date')['Close'].to_dict()
        matched = []
        for date, pred_price in forecast:
            if date in actual_dict:
                matched.append({
                    'Date': date,
                    'Predicted': pred_price,
                    'Actual': actual_dict[date]
                })

        if not matched:
            log("‚ö†Ô∏è Kh√¥ng c√≥ ng√†y n√†o tr√πng gi·ªØa d·ª± b√°o v√† d·ªØ li·ªáu th·ª±c t·∫ø.")
            return {"error": "Kh√¥ng c√≥ d·ªØ li·ªáu th·ª±c t·∫ø tr√πng v·ªõi d·ª± b√°o."}

        result_df = pd.DataFrame(matched).sort_values('Date')

        mae = mean_absolute_error(result_df['Actual'], result_df['Predicted'])
        rmse = np.sqrt(mean_squared_error(result_df['Actual'], result_df['Predicted']))
        mape = np.mean(np.abs((result_df['Actual'] - result_df['Predicted']) / result_df['Actual'])) * 100

        log(f"üìà MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%")

        fig = go.Figure()
        fig.add_trace(go.Scatter(x=result_df['Date'], y=result_df['Actual'], name='Actual', mode='lines+markers'))
        fig.add_trace(go.Scatter(x=result_df['Date'], y=result_df['Predicted'], name='Forecast', mode='lines+markers', line=dict(dash='dot')))
        fig.update_layout(title=f"Forecast vs Actual - {symbol}",
                          xaxis_title="Date", yaxis_title="Price",
                          template="plotly_white")
        chart_html = fig.to_html(full_html=False, include_plotlyjs='cdn')

        return {
            "symbol": symbol,
            "model_type": model_type.upper(),
            "steps": steps,
            "mae": round(mae, 4),
            "rmse": round(rmse, 4),
            "mape": round(mape, 2),
            "chart_html": chart_html
        }

    except Exception as e:
        log(f"‚ùå L·ªói khi ƒë√°nh gi√° m√¥ h√¨nh: {str(e)}")
        return {"error": str(e)}
